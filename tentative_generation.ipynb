{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Poetry database. Check the cleaning_data.ipynb notebook for more details about the data collection.\n",
    "url = \"https://raw.githubusercontent.com/remi-vidal/NLP-ensae/main/df_cleaned.csv\"\n",
    "df = pd.read_csv(url, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0#UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<pad>']#['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonely_content = df[df.theme == 'lonely'].content\n",
    "\n",
    "parsed_data = lonely_content.apply(lambda x: x.lower().split(\"\\n\"))\n",
    "\n",
    "corpus = []\n",
    "for row in parsed_data:\n",
    "  corpus.extend(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (60.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.22.0)\n",
      "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "import string\n",
    "\n",
    "corpus = [text.translate(str.maketrans('', '', string.punctuation)) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2058/2058 [00:13<00:00, 148.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "tok = en_core_web_sm.load()#spacy.load(\"en_core_web_sm\")#get_tokenizer('spacy', language='en')#TweetTokenizer()\n",
    "\n",
    "# stoi string to index\n",
    "# itos index to string\n",
    "\n",
    "VOC = {'stoi': {}, 'counts': {}, 'nbwords': 0}\n",
    "\n",
    "def parse_entry(entry):\n",
    "  tokens = tok(entry.lower().strip())#tok.tokenize(entry.lower().strip())\n",
    "  for t in tokens:\n",
    "    t = str(t)\n",
    "    if t not in VOC['stoi']: \n",
    "      VOC['stoi'][t] = VOC['nbwords']\n",
    "      VOC['counts'][t] = 1\n",
    "      VOC['nbwords'] += 1\n",
    "    else:\n",
    "      VOC['counts'][t] = VOC['counts'][t] + 1\n",
    "\n",
    "\n",
    "for t in special_symbols: \n",
    "  VOC['stoi'][t] = VOC['nbwords']\n",
    "  VOC['counts'][t] = 1\n",
    "  VOC['nbwords'] += 1\n",
    "\n",
    "for entry in tqdm(corpus): \n",
    "  parse_entry(entry)\n",
    "\n",
    "VOC['itos'] = { v:k for k,v in VOC['stoi'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(corpus, vocab):\n",
    "    data = list()\n",
    "    for text in corpus:\n",
    "        token_list = [vocab['stoi'][str(token)] for token in tok(text.lower().strip())]#tok.tokenize(text.lower().strip())]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_seq = torch.tensor(token_list[:i+1], dtype=torch.long)\n",
    "            data.append(n_gram_seq)\n",
    "    return data\n",
    "\n",
    "train_data = data_process(corpus, VOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i[:-1] for i in train_data]   # taking all the words except the last in the input set\n",
    "y = [i[-1] for i in train_data]    # taking last words in the output set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"custom dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.maxlen = 0\n",
    "        for u in X:\n",
    "          self.maxlen = max(self.maxlen, len(u))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      # We pad the X dynamically, so that it has the same length\n",
    "\n",
    "        sample = F.pad(self.X[idx], (0, self.maxlen - len(self.X[idx])), \"constant\", VOC['stoi']['<pad>']), self.y[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X, y)#TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = max(VOC['stoi'].values())+1\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, X, h=None, c=None):\n",
    "        if h is None:\n",
    "            h, c = self.init_state(X.size(0))\n",
    "        out = self.embedding(X)\n",
    "        out, (h, c) = self.lstm(out, (h, c))\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc1(out)\n",
    "        out = out.view(-1, X.size(1), self.vocab_size)\n",
    "        out = out[:, -1]\n",
    "        \n",
    "        return out, h, c\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        num_l = self.num_layers\n",
    "        hidden = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)\n",
    "        cell = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss:6.804762363433838\n",
      "Epoch: 6 Loss:6.373279094696045\n",
      "Epoch: 11 Loss:5.57279109954834\n",
      "Epoch: 16 Loss:4.659511089324951\n",
      "Epoch: 21 Loss:3.8566181659698486\n",
      "Epoch: 26 Loss:3.096052646636963\n",
      "Epoch: 31 Loss:2.3997631072998047\n",
      "Epoch: 36 Loss:1.7979499101638794\n",
      "Epoch: 41 Loss:1.3026727437973022\n",
      "Epoch: 46 Loss:1.0171420574188232\n",
      "Epoch: 51 Loss:0.8731924891471863\n",
      "Epoch: 56 Loss:0.7847435474395752\n",
      "Epoch: 61 Loss:0.7471028566360474\n",
      "Epoch: 66 Loss:0.6975546479225159\n",
      "Epoch: 71 Loss:0.6815626621246338\n",
      "Epoch: 76 Loss:0.6685712933540344\n",
      "Epoch: 81 Loss:0.6910117268562317\n",
      "Epoch: 86 Loss:0.6432894468307495\n",
      "Epoch: 91 Loss:0.6282094717025757\n",
      "Epoch: 96 Loss:0.6376839280128479\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, h, c = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5) # Clipping Gradients\n",
    "        optimizer.step()\n",
    "    if epoch%5 == 0:\n",
    "        print(f\"Epoch: {epoch+1} Loss:{epoch_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"loneliness_nopunc_256.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-114-ccef973ec82d>:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  idx = torch.multinomial(nn.Softmax()(out.flatten()), 1)#torch.argmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am such a lonely \n",
      "\n",
      "man hiss happily existence feeling \n",
      "\n",
      "visitors think always room feeling \n",
      "\n",
      "want suddenly glide imported gleam \n",
      "\n",
      "gosh rhyme speak frames cloudy \n",
      "\n",
      "comfort ones crown jesus dear \n",
      "\n",
      "enlightenment merely shine stared air \n",
      "\n",
      "meant shut touch help manage \n",
      "\n",
      "gleam creep shoulderlength speak blood \n",
      "\n",
      "bittersweet hates glinting drown younger \n",
      "\n",
      "looking hand mouths sipping feeling \n",
      "\n",
      "disappear "
     ]
    }
   ],
   "source": [
    "seed_text = \"i am such a lonely man\" #Starting of a song\n",
    "next_words = 50\n",
    "\n",
    "for i in range(next_words):\n",
    "    token_list = np.ones(21, dtype=int)\n",
    "    text_token = np.array([VOC['stoi'][str(token)] for token in tok(seed_text)])\n",
    "    if len(text_token)>21:text_token = text_token[-21:]\n",
    "    token_list[:len(text_token)] = text_token\n",
    "    token_list = torch.from_numpy(token_list).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    out,h,c = model(token_list)\n",
    "    \n",
    "    idx = torch.multinomial(nn.Softmax()(out.flatten()), 1)#torch.argmax(out)\n",
    "    seed_text += \" \" + VOC['itos'][int(idx)]\n",
    "    \n",
    "for i,word in enumerate(seed_text.split()):\n",
    "    print(word,end=\" \"),\n",
    "    if i!=0 and (i+1)%5==0:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
